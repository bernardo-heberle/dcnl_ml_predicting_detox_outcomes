{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import regex as re\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from lightgbm import LGBMClassifier\n",
    "import math\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: random_forest_grid\n",
    "\n",
    "Purpose: Performing hyperparameter search for random forest classifier using 5X nested 5-fold Cross Validation.\n",
    "\n",
    "Input: X and y (Features and Outcome)\n",
    "\n",
    "Output: Results of the hyperparameter GridSearchCV\n",
    "'''\n",
    "\n",
    "def random_forest_grid(X, y):\n",
    "    \n",
    "    rkf = RepeatedKFold(n_splits=5, n_repeats=5, random_state=2022)\n",
    "    \n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "    [\n",
    "     ('model', RandomForestClassifier(random_state=2022))\n",
    "    ]\n",
    "    )\n",
    "    \n",
    "    search = GridSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_grid = {\n",
    "      'model__min_samples_leaf':np.arange(1, 11, 1),\n",
    "      'model__criterion':[\"gini\",\"entropy\"],\n",
    "      'model__n_estimators':np.arange(11, 752, 10),\n",
    "      'model__bootstrap':[True, False]        \n",
    "     },\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=rkf,\n",
    "    verbose=3\n",
    "    )\n",
    "    \n",
    "    search.fit(X,y)\n",
    "    \n",
    "    return search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: logistic_regression_grid\n",
    "\n",
    "Purpose: Performing hyperparameter search for logistic regression classifier using 5X nested 5-fold Cross Validation.\n",
    "\n",
    "Input: X and y (Features and Outcome)\n",
    "\n",
    "Output: Results of the hyperparameter GridSearchCV\n",
    "'''\n",
    "\n",
    "def logistic_regression_grid(X, y):\n",
    "    \n",
    "    \n",
    "    rkf = RepeatedKFold(n_splits=5, n_repeats=5, random_state=2022)\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "    [\n",
    "     ('model', LogisticRegression(random_state=2022, max_iter=10000))\n",
    "    ]\n",
    "    )\n",
    "    \n",
    "    search_1 = GridSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_grid = {\n",
    "      'model__penalty':[\"l2\"],\n",
    "      'model__C':np.arange(0.01, 1.01, 0.01),\n",
    "      'model__solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "      'model__fit_intercept':[True, False]        \n",
    "     },\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=rkf,\n",
    "    verbose=3\n",
    "    )\n",
    "    \n",
    "    search_2 = GridSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_grid = {\n",
    "      'model__penalty':[\"l1\"],\n",
    "      'model__C':np.arange(0.01, 1.01, 0.01),\n",
    "      'model__solver':['liblinear', 'saga'],\n",
    "      'model__fit_intercept':[True, False]        \n",
    "     },\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=rkf,\n",
    "    verbose=3\n",
    "    )\n",
    "    \n",
    "    search_3 = GridSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_grid = {\n",
    "      'model__penalty':[\"elasticnet\"],\n",
    "      'model__C':np.arange(0.01, 1.01, 0.01),\n",
    "      'model__l1_ratio': np.arange(0.1, 1, 0.1),\n",
    "      'model__solver':['saga'],\n",
    "      'model__fit_intercept':[True, False]        \n",
    "     },\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=rkf,\n",
    "    verbose=3\n",
    "    )\n",
    "    \n",
    "    search_4 = GridSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_grid = {\n",
    "      'model__penalty':[\"none\"],\n",
    "      'model__solver':['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "      'model__fit_intercept':[True, False]        \n",
    "     },\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=rkf,\n",
    "    verbose=3\n",
    "    )\n",
    "    \n",
    "    search_1.fit(X,y)\n",
    "    search_2.fit(X,y)\n",
    "    search_3.fit(X,y)\n",
    "    search_4.fit(X,y)\n",
    "    \n",
    "    \n",
    "    if (((search_1.best_score_ > search_2.best_score_) & (search_1.best_score_ > search_3.best_score_)) & (search_1.best_score_ > search_4.best_score_)):\n",
    "        return search_1\n",
    "    \n",
    "    elif ((search_2.best_score_ > search_3.best_score_) & (search_2.best_score_ > search_4.best_score_)):\n",
    "        return search_2\n",
    "    \n",
    "    elif (search_3.best_score_ > search_4.best_score_):\n",
    "        return search_3\n",
    "    \n",
    "    else:\n",
    "        return search_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: support_vector_machine_grid\n",
    "\n",
    "Purpose: Performing hyperparameter search for support vector machine classifier using 5X nested 5-fold Cross Validation.\n",
    "\n",
    "Input: X and y (Features and Outcome)\n",
    "\n",
    "Output: Results of the hyperparameter GridSearchCV\n",
    "'''\n",
    "\n",
    "def support_vector_machine_grid(X, y):\n",
    "    \n",
    "    rkf = RepeatedKFold(n_splits=5, n_repeats=5, random_state=2022)\n",
    "    \n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "    [\n",
    "     ('model', SVC(random_state=2022))\n",
    "    ]\n",
    "    )\n",
    "    \n",
    "    search_1 = GridSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_grid = {\n",
    "      'model__gamma': ['scale', 'auto'],\n",
    "      'model__coef0': [0, 1, 2],\n",
    "      'model__C': np.arange(0.01, 1.001, 0.01),\n",
    "      'model__kernel': ['poly'],\n",
    "      'model__degree': [2, 3],\n",
    "     },\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=rkf,\n",
    "    verbose=3\n",
    "    )\n",
    "        \n",
    "    search_2 = GridSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_grid = {\n",
    "      'model__gamma': ['scale', 'auto'],\n",
    "      'model__coef0': [0, 0.5, 1, 1.5, 2],\n",
    "      'model__C': np.arange(0.01, 1.001, 0.01),\n",
    "      'model__kernel': ['sigmoid'],\n",
    "     },\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=rkf,\n",
    "    verbose=3\n",
    "    )\n",
    "        \n",
    "    search_3 = GridSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_grid = {\n",
    "      'model__gamma': ['scale', 'auto'],\n",
    "      'model__C': np.arange(0.005, 1.001, 0.005),\n",
    "      'model__kernel': ['linear', 'rbf'],\n",
    "     },\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=rkf,\n",
    "    verbose=3\n",
    "    )\n",
    "    \n",
    "    search_1.fit(X,y)\n",
    "    search_2.fit(X,y)\n",
    "    search_3.fit(X,y)\n",
    "        \n",
    "    if ((search_1.best_score_ > search_2.best_score_) & (search_1.best_score_ > search_3.best_score_)):\n",
    "        return search_1\n",
    "        \n",
    "    elif (search_2.best_score_ > search_3.best_score_):\n",
    "        return search_2\n",
    "        \n",
    "    else:\n",
    "        return search_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: KNN_grid\n",
    "\n",
    "Purpose: Performing hyperparameter search for K-Nearest Neighbors classifier using 5X nested 5-fold Cross Validation.\n",
    "\n",
    "Input: X and y (Features and Outcome)\n",
    "\n",
    "Output: Results of the hyperparameter GridSearchCV\n",
    "'''\n",
    "\n",
    "def KNN_grid(X, y):\n",
    "    \n",
    "    rkf = RepeatedKFold(n_splits=5, n_repeats=5, random_state=2022)\n",
    "    \n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "    [\n",
    "     ('model', KNeighborsClassifier())\n",
    "    ]\n",
    "    )\n",
    "    \n",
    "    search = GridSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_grid = {\n",
    "      'model__n_neighbors':np.arange(1, 128, 2),\n",
    "      'model__weights':[\"uniform\",\"distance\"],\n",
    "      'model__leaf_size':np.arange(1, 33, 3),\n",
    "      'model__p':[1, 2]        \n",
    "     },\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=rkf,\n",
    "    verbose=3\n",
    "    )\n",
    "    \n",
    "    search.fit(X,y)\n",
    "    \n",
    "    return search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: naive_bayes_grid\n",
    "\n",
    "Purpose: Performing hyperparameter search for Naive Bayes classifier using 5X nested 5-fold Cross Validation.\n",
    "\n",
    "Input: X and y (Features and Outcome)\n",
    "\n",
    "Output: Results of the hyperparameter GridSearchCV\n",
    "'''\n",
    "\n",
    "def naive_bayes_grid(X, y):\n",
    "    \n",
    "    rkf = RepeatedKFold(n_splits=5, n_repeats=5, random_state=2022)\n",
    "    \n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "    [\n",
    "     ('model', GaussianNB())\n",
    "    ]\n",
    "    )\n",
    "    \n",
    "    search = GridSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_grid = {\n",
    "      'model__var_smoothing': np.logspace(0,-11, num=3000)    \n",
    "     },\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=rkf,\n",
    "    verbose=3\n",
    "    )\n",
    "    \n",
    "    search.fit(X,y)\n",
    "    \n",
    "    return search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: light_gbm_grid\n",
    "\n",
    "Purpose: Performing hyperparameter search for Light Gradient Boosting classifier using 5X nested 5-fold Cross Validation.\n",
    "\n",
    "Input: X and y (Features and Outcome)\n",
    "\n",
    "Output: Results of the hyperparameter GridSearchCV\n",
    "'''\n",
    "\n",
    "def light_gbm_grid(X, y):\n",
    "    \n",
    "    rkf = RepeatedKFold(n_splits=5, n_repeats=5, random_state=2022)\n",
    "    \n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "    [\n",
    "     ('model', LGBMClassifier(random_state=2022))\n",
    "    ]\n",
    "    )\n",
    "    \n",
    "    search = GridSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_grid = {\n",
    "      'model__n_estimators': [20, 100, 400],\n",
    "      'model__learning_rate': [0.005, 0.01, 0.05, 0.1, 0.3, 0.5],\n",
    "      'model__num_leaves': [5, 10, 20, 35, 50, 75],\n",
    "      'model__min_child_samples':[5, 10, 20, 30, 50, 75],\n",
    "      'model__max_bin': [20, 50, 100, 255, 400]\n",
    "     },\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=rkf,\n",
    "    verbose=3\n",
    "    )\n",
    "    \n",
    "    search.fit(X,y)\n",
    "    \n",
    "    return search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dataframe for 70_30 feature selected data.\n",
    "\n",
    "name = \"../data/feature_selected_train_dataset_70_30.csv\"\n",
    "df = pd.read_csv(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate into features and outcomes\n",
    "\n",
    "X_train = df.drop(['outcome'], axis=1, inplace=False)\n",
    "y_train = df[\"outcome\"]\n",
    "\n",
    "## Scale X with standard scaler for models that require it.\n",
    "X_train_scaled = StandardScaler().fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 25 folds for each of 3000 candidates, totalling 75000 fits\n",
      "Accuracy of the best random forest model was: 0.7560593792172741\n",
      "Hyperparameters of best random forest model were: \n",
      "\n",
      " {'model__bootstrap': True, 'model__criterion': 'gini', 'model__min_samples_leaf': 1, 'model__n_estimators': 191}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../top_models/best_random_forest.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Perform hyperparameter search for random forest\n",
    "search_rf = random_forest_grid(X_train, y_train)\n",
    "\n",
    "## Display best accuracy score\n",
    "print(\"Accuracy of the best random forest model was:\", search_rf.best_score_)\n",
    "\n",
    "## Display parameters for best model\n",
    "print(\"Hyperparameters of best random forest model were: \\n\\n\", search_rf.best_params_)\n",
    "\n",
    "## Save best random forest classifier model for future usage\n",
    "joblib.dump(search_rf.best_estimator_, '../top_models/best_random_forest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 25 folds for each of 1000 candidates, totalling 25000 fits\n",
      "Fitting 25 folds for each of 400 candidates, totalling 10000 fits\n",
      "Fitting 25 folds for each of 1800 candidates, totalling 45000 fits\n",
      "Fitting 25 folds for each of 8 candidates, totalling 200 fits\n",
      "Accuracy of the best logistic regression model was: 0.7737651821862349\n",
      "Hyperparameters of best logistic regression model were: \n",
      "\n",
      " {'model__C': 0.02, 'model__fit_intercept': False, 'model__l1_ratio': 0.1, 'model__penalty': 'elasticnet', 'model__solver': 'saga'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../top_models/best_logistic_regression.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Perform hyperparameter search for logistic regression\n",
    "search_logreg = logistic_regression_grid(X_train_scaled, y_train)\n",
    "\n",
    "## Display best accuracy score\n",
    "print(\"Accuracy of the best logistic regression model was:\", search_logreg.best_score_)\n",
    "\n",
    "## Display parameters for best model\n",
    "print(\"Hyperparameters of best logistic regression model were: \\n\\n\", search_logreg.best_params_)\n",
    "\n",
    "## Save best Logistic Regression classifier model for future usage\n",
    "joblib.dump(search_logreg.best_estimator_, '../top_models/best_logistic_regression.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 25 folds for each of 1200 candidates, totalling 30000 fits\n",
      "Fitting 25 folds for each of 1000 candidates, totalling 25000 fits\n",
      "Fitting 25 folds for each of 800 candidates, totalling 20000 fits\n",
      "Accuracy of the best support vector machine model was: 0.7780026990553306\n",
      "Hyperparameters of best support vector machine model were: \n",
      "\n",
      " {'model__C': 0.29000000000000004, 'model__coef0': 0.5, 'model__gamma': 'auto', 'model__kernel': 'sigmoid'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../top_models/best_support_vector_machine.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Perform hyperparameter search for logistic regression\n",
    "search_svc = support_vector_machine_grid(X_train_scaled, y_train)\n",
    "\n",
    "## Display best accuracy score\n",
    "print(\"Accuracy of the best support vector machine model was:\", search_svc.best_score_)\n",
    "\n",
    "## Display parameters for best model\n",
    "print(\"Hyperparameters of best support vector machine model were: \\n\\n\", search_svc.best_params_)\n",
    "\n",
    "## Save best Support Vector Machine classifier model for future usage\n",
    "joblib.dump(search_svc.best_estimator_, '../top_models/best_support_vector_machine.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 25 folds for each of 2816 candidates, totalling 70400 fits\n",
      "Accuracy of the best K-Nearest Neighbor model was: 0.7676113360323886\n",
      "Hyperparameters of best K-Nearest Neighbor model were: \n",
      "\n",
      " {'model__leaf_size': 1, 'model__n_neighbors': 75, 'model__p': 2, 'model__weights': 'distance'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../top_models/best_KNN.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Perform hyperparameter search for logistic regression\n",
    "search_knn = KNN_grid(X_train_scaled, y_train)\n",
    "\n",
    "## Display best accuracy score\n",
    "print(\"Accuracy of the best K-Nearest Neighbor model was:\", search_knn.best_score_)\n",
    "\n",
    "## Display parameters for best model\n",
    "print(\"Hyperparameters of best K-Nearest Neighbor model were: \\n\\n\", search_knn.best_params_)\n",
    "\n",
    "## Save best KNN classifier model for future usage\n",
    "joblib.dump(search_knn.best_estimator_, '../top_models/best_KNN.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 25 folds for each of 3000 candidates, totalling 75000 fits\n",
      "Accuracy of the best Naive Bayes model was: 0.7406207827260458\n",
      "Hyperparameters of best Naive Bayes model were: \n",
      "\n",
      " {'model__var_smoothing': 4.126088252678016e-06}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../top_models/best_naive_bayes.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Perform hyperparameter search for logistic regression\n",
    "search_naive_bayes = naive_bayes_grid(X_train, y_train)\n",
    "\n",
    "## Display best accuracy score\n",
    "print(\"Accuracy of the best Naive Bayes model was:\", search_naive_bayes.best_score_)\n",
    "\n",
    "## Display parameters for best model\n",
    "print(\"Hyperparameters of best Naive Bayes model were: \\n\\n\", search_naive_bayes.best_params_)\n",
    "\n",
    "## Save best Naive Bayes classifier model for future usage\n",
    "joblib.dump(search_naive_bayes.best_estimator_, '../top_models/best_naive_bayes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 25 folds for each of 3240 candidates, totalling 81000 fits\n",
      "Accuracy of the best Light GBM model was: 0.7406477732793522\n",
      "Hyperparameters of best Light GBM model were: \n",
      "\n",
      " {'model__learning_rate': 0.005, 'model__max_bin': 20, 'model__min_child_samples': 20, 'model__n_estimators': 400, 'model__num_leaves': 10}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../top_models/best_light_gbm.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Perform hyperparameter search for logistic regression\n",
    "search_light_gbm = light_gbm_grid(X_train, y_train)\n",
    "\n",
    "## Display best accuracy score\n",
    "print(\"Accuracy of the best Light GBM model was:\", search_light_gbm.best_score_)\n",
    "\n",
    "## Display parameters for best model\n",
    "print(\"Hyperparameters of best Light GBM model were: \\n\\n\", search_light_gbm.best_params_)\n",
    "\n",
    "## Save best Light GBM classifier model for future usage\n",
    "joblib.dump(search_light_gbm.best_estimator_, '../top_models/best_light_gbm.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
