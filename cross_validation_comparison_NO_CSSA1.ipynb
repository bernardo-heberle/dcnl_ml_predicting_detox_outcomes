{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import regex as re\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from lightgbm import LGBMClassifier\n",
    "import math\n",
    "from sklearn.model_selection import cross_validate\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function name: cross_validation_comparison\n",
    "\n",
    "Input: List of models, list of model names, raw feature values, scaled feature values, outcome values.\n",
    "\n",
    "Output: Print result of cross validation comparison on screen and return the string of the results.\n",
    "\n",
    "Purpose: Compare cross validation results between the best model for each classifier.\n",
    "\n",
    "'''\n",
    "\n",
    "def cross_validation_comparison(list_models, list_model_names, X_raw, X_scaled, y):\n",
    "\n",
    "    ## Create variable for model report\n",
    "    final_model_report = \"\"\n",
    "\n",
    "    ## Loop through models\n",
    "    for i in range(6):\n",
    "        \n",
    "        ## Define model and model name for this loop\n",
    "        model_name = list_model_names[i]\n",
    "        model = list_models[i]\n",
    "        \n",
    "        ## Define arrays for storing average values of each metric on this loop\n",
    "        average_scores_accuracy = np.array([])\n",
    "        average_std_accuracy = np.array([])\n",
    "        average_scores_recall = np.array([])\n",
    "        average_std_recall = np.array([])\n",
    "        average_scores_precision = np.array([])\n",
    "        average_std_precision = np.array([])\n",
    "        average_scores_roc = np.array([])\n",
    "        average_std_roc = np.array([])\n",
    "        average_scores_f1 = np.array([])\n",
    "        average_std_f1 = np.array([])\n",
    "        \n",
    "        ## Loop through 20 rounds of 5-fold cross validation for current model being tested\n",
    "        for j in range(20):\n",
    "            \n",
    "            ## Check if model needs scaled data or raw data\n",
    "            if model_name in [\"LOGISTIC REGRESSION\", \"SUPPORT VECTOR MACHINE\", \"K NEAREST NEIGHBOR\"]:\n",
    "                X = X_scaled\n",
    "            else:\n",
    "                X = X_raw\n",
    "\n",
    "            ## Run 5-fold cross validation\n",
    "            cv_results = cross_validate(model, X, y, cv=5, scoring=('accuracy', 'recall', 'precision', 'f1', 'roc_auc'))\n",
    "\n",
    "            ## Store metrics of cross validation scores in variables\n",
    "            scores_accuracy = cv_results['test_accuracy']\n",
    "            scores_recall = cv_results['test_recall']\n",
    "            scores_precision = cv_results['test_precision']\n",
    "            scores_roc = cv_results['test_roc_auc']\n",
    "            scores_f1 = cv_results['test_f1']\n",
    "\n",
    "            ## Store standard deviation for cross validation scores\n",
    "            std_accuracy = float(scores_accuracy.std(ddof=1))\n",
    "            std_recall = float(scores_recall.std(ddof=1))\n",
    "            std_precision = float(scores_precision.std(ddof=1))\n",
    "            std_roc = float(scores_roc.std(ddof=1))\n",
    "            std_f1 = float(scores_f1.std(ddof=1))\n",
    "\n",
    "            ## Append mean of metric accross cross-validation to the list of average scores for that metric\n",
    "            average_scores_accuracy = np.append(average_scores_accuracy, scores_accuracy.mean())\n",
    "            \n",
    "            ## Append standard deviation of cross validation metric to list of standard deviations\n",
    "            average_std_accuracy = np.append(average_std_accuracy, std_accuracy)\n",
    "            \n",
    "            ## Repeat same process for all other metrics\n",
    "            average_scores_recall = np.append(average_scores_recall, scores_recall.mean())\n",
    "            average_std_recall = np.append(average_std_recall, std_recall)\n",
    "\n",
    "            average_scores_precision = np.append(average_scores_precision, scores_precision.mean())\n",
    "            average_std_precision = np.append(average_std_precision, std_precision)\n",
    "\n",
    "            average_scores_roc = np.append(average_scores_roc, scores_roc.mean())\n",
    "            average_std_roc = np.append(average_std_roc, std_roc)\n",
    "\n",
    "            average_scores_f1 = np.append(average_scores_f1, scores_f1.mean())\n",
    "            average_std_f1 = np.append(average_std_f1, std_f1)\n",
    "\n",
    "\n",
    "        ## Print averages of average score lists on the screen once the loop for the model is done running\n",
    "        print(\"\\n\\n\\n |||||||||||||||||| \" + model_name + \" |||||||||||||||||| \\n\\n\")\n",
    "        print(\"Accuracy Score:\", round(average_scores_accuracy.mean(), 3), \"    STD:\", round(average_std_accuracy.mean(), 3), \"    SEM:\", round((average_std_accuracy.mean()/math.sqrt(10)), 3))\n",
    "        print(\"F1 Score:\", round(average_scores_f1.mean(),3), \"    STD:\", round(average_std_f1.mean(), 3), \"    SEM:\", round((average_std_f1.mean()/math.sqrt(10)), 3))\n",
    "        print(\"Recall Score:\", round(average_scores_recall.mean(), 3), \"    STD:\", round(average_std_recall.mean(), 3), \"    SEM:\", round((average_std_recall.mean()/math.sqrt(10)), 3))\n",
    "        print(\"Precision Score:\", round(average_scores_precision.mean(), 3), \"    STD:\", round(average_std_precision.mean(), 3), \"    SEM:\", round((average_std_precision.mean()/math.sqrt(10)), 3))\n",
    "        print(\"Roc AUC Score:\", round(average_scores_roc.mean(), 3), \"    STD:\", round(average_std_roc.mean(), 3), \"    SEM:\", round((average_std_roc.mean()/math.sqrt(10)), 3))\n",
    "        \n",
    "        ## Create string with the scores for this models\n",
    "        current_model_report = str(\"\\n\\n\\n |||||||||||||||||| \" + model_name + \" |||||||||||||||||| \\n\\n\" \n",
    "                         + \"Accuracy Score: \" + str(round(average_scores_accuracy.mean(), 3)) + \"     STD: \" + str(round(average_std_accuracy.mean(), 3)) + \"     SEM: \" + str(round((average_std_accuracy.mean()/math.sqrt(10)), 3)) + \"\\n\" \n",
    "                         + \"F1 Score: \" + str(round(average_scores_f1.mean(), 3)) + \"     STD: \" + str(round(average_std_f1.mean(), 3)) + \"     SEM: \" + str(round((average_std_f1.mean()/math.sqrt(10)), 3)) + \"\\n\" \n",
    "                         + \"Recall Score: \" + str(round(average_scores_recall.mean(), 3)) + \"     STD: \" + str(round(average_std_recall.mean(), 3)) + \"     SEM: \" + str(round((average_std_recall.mean()/math.sqrt(10)), 3)) + \"\\n\" \n",
    "                         + \"Precision Score: \" + str(round(average_scores_precision.mean(), 3)) + \"     STD: \" + str(round(average_std_precision.mean(), 3)) + \"     SEM: \" + str(round((average_std_precision.mean()/math.sqrt(10)), 3)) + \"\\n\" \n",
    "                         + \"ROC AUC Score: \" + str(round(average_scores_roc.mean(), 3)) + \"     STD: \" + str(round(average_std_roc.mean(), 3)) + \"     SEM: \" + str(round((average_std_roc.mean()/math.sqrt(10)), 3)) + \"\\n\" )\n",
    "        \n",
    "        ## Add model scores to final report\n",
    "        final_model_report = final_model_report + current_model_report\n",
    "    \n",
    "    ## Return final report once everything is done\n",
    "    return final_model_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dataframe for 70_30 feature selected data.\n",
    "name = \"../data/feature_selected_train_dataset_70_30.csv\"\n",
    "df = pd.read_csv(name)\n",
    "\n",
    "## Separate into features and outcomes\n",
    "X_train = df.drop(['outcome', \"CSSA Score Week 1\"], axis=1, inplace=False)\n",
    "y_train = df[\"outcome\"]\n",
    "\n",
    "## Scale X with standard scaler for models that require it.\n",
    "X_train_scaled = StandardScaler().fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load all models\n",
    "random_forest = joblib.load(\"../top_models/best_random_forest.pkl\")\n",
    "logistic_regression = joblib.load(\"../top_models/best_logistic_regression.pkl\")\n",
    "support_vector_machine = joblib.load(\"../top_models/best_support_vector_machine.pkl\")\n",
    "k_nearest_neighbors = joblib.load(\"../top_models/best_KNN.pkl\")\n",
    "naive_bayes = joblib.load(\"../top_models/best_naive_bayes.pkl\")\n",
    "light_gbm  = joblib.load(\"../top_models/best_light_gbm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('model',\n",
       "                 LGBMClassifier(learning_rate=0.005, max_bin=20,\n",
       "                                n_estimators=400, num_leaves=10,\n",
       "                                random_state=2022))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Refit all models to data without the CSSA1 Score\n",
    "random_forest.fit(X_train, y_train)\n",
    "logistic_regression.fit(X_train_scaled, y_train)\n",
    "support_vector_machine.fit(X_train_scaled, y_train)\n",
    "k_nearest_neighbors.fit(X_train_scaled, y_train)\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "light_gbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create list of models and list of model names\n",
    "list_models = [random_forest, logistic_regression, support_vector_machine, k_nearest_neighbors, naive_bayes, light_gbm]\n",
    "list_model_names = [\"RANDOM FOREST\", \"LOGISTIC REGRESSION\", \"SUPPORT VECTOR MACHINE\",\n",
    "                    \"K NEAREST NEIGHBOR\", \"NAIVE BAYES\", \"LIGHT GBM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " |||||||||||||||||| RANDOM FOREST |||||||||||||||||| \n",
      "\n",
      "\n",
      "Accuracy Score: 0.677     STD: 0.07     SEM: 0.022\n",
      "F1 Score: 0.67     STD: 0.08     SEM: 0.025\n",
      "Recall Score: 0.674     STD: 0.126     SEM: 0.04\n",
      "Precision Score: 0.678     STD: 0.07     SEM: 0.022\n",
      "Roc AUC Score: 0.733     STD: 0.06     SEM: 0.019\n",
      "\n",
      "\n",
      "\n",
      " |||||||||||||||||| LOGISTIC REGRESSION |||||||||||||||||| \n",
      "\n",
      "\n",
      "Accuracy Score: 0.683     STD: 0.064     SEM: 0.02\n",
      "F1 Score: 0.669     STD: 0.09     SEM: 0.028\n",
      "Recall Score: 0.663     STD: 0.137     SEM: 0.043\n",
      "Precision Score: 0.683     STD: 0.059     SEM: 0.019\n",
      "Roc AUC Score: 0.762     STD: 0.079     SEM: 0.025\n",
      "\n",
      "\n",
      "\n",
      " |||||||||||||||||| SUPPORT VECTOR MACHINE |||||||||||||||||| \n",
      "\n",
      "\n",
      "Accuracy Score: 0.698     STD: 0.074     SEM: 0.023\n",
      "F1 Score: 0.664     STD: 0.089     SEM: 0.028\n",
      "Recall Score: 0.611     STD: 0.115     SEM: 0.036\n",
      "Precision Score: 0.739     STD: 0.09     SEM: 0.028\n",
      "Roc AUC Score: 0.761     STD: 0.087     SEM: 0.028\n",
      "\n",
      "\n",
      "\n",
      " |||||||||||||||||| K NEAREST NEIGHBOR |||||||||||||||||| \n",
      "\n",
      "\n",
      "Accuracy Score: 0.693     STD: 0.109     SEM: 0.034\n",
      "F1 Score: 0.68     STD: 0.119     SEM: 0.038\n",
      "Recall Score: 0.674     STD: 0.164     SEM: 0.052\n",
      "Precision Score: 0.698     STD: 0.098     SEM: 0.031\n",
      "Roc AUC Score: 0.766     STD: 0.073     SEM: 0.023\n",
      "\n",
      "\n",
      "\n",
      " |||||||||||||||||| NAIVE BAYES |||||||||||||||||| \n",
      "\n",
      "\n",
      "Accuracy Score: 0.636     STD: 0.103     SEM: 0.032\n",
      "F1 Score: 0.575     STD: 0.143     SEM: 0.045\n",
      "Recall Score: 0.516     STD: 0.172     SEM: 0.054\n",
      "Precision Score: 0.682     STD: 0.161     SEM: 0.051\n",
      "Roc AUC Score: 0.726     STD: 0.069     SEM: 0.022\n",
      "\n",
      "\n",
      "\n",
      " |||||||||||||||||| LIGHT GBM |||||||||||||||||| \n",
      "\n",
      "\n",
      "Accuracy Score: 0.677     STD: 0.083     SEM: 0.026\n",
      "F1 Score: 0.669     STD: 0.082     SEM: 0.026\n",
      "Recall Score: 0.663     STD: 0.109     SEM: 0.035\n",
      "Precision Score: 0.687     STD: 0.097     SEM: 0.031\n",
      "Roc AUC Score: 0.742     STD: 0.096     SEM: 0.03\n"
     ]
    }
   ],
   "source": [
    "## Get cross validation results and print them on screen\n",
    "cv_results = cross_validation_comparison(list_models, list_model_names, X_train, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save report to txt file\n",
    "text_file = open(\"../reports/cross_validation_results_NO_CSSA1_70_30.txt\", \"w\")\n",
    "n = text_file.write(cv_results)\n",
    "text_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
