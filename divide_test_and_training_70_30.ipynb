{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: outcome_split\n",
    "\n",
    "Purpose: Make the outcome variable binary, splitting it  within the percentiles\n",
    "of previously inputed low and high tails. \n",
    "\n",
    "Input: Dataframe, outcome variable name, low tail value, high tail value\n",
    "\n",
    "Output: Dataframe with \"outcome\" variable split in 1s and 0s as asked by the user\n",
    "\n",
    "'''\n",
    "\n",
    "def outcome_split(df, outcome, low_tail, high_tail):\n",
    "    \n",
    "    ## Create percentiles\n",
    "\n",
    "    my_series = df[outcome]\n",
    "    my_series = my_series.values.tolist()\n",
    "    my_series = np.array(my_series)\n",
    "\n",
    "    q1, q3 = np.percentile(my_series, [low_tail, high_tail])\n",
    "    \n",
    "    ## Divide by percentiles\n",
    "\n",
    "    constraint = df[outcome] >= q3\n",
    "    constraint2 = df[outcome] < q1\n",
    "\n",
    "\n",
    "    df.loc[constraint, 'outcome'] = int(1)\n",
    "    df.loc[constraint2, 'outcome'] = int(0)\n",
    "    \n",
    "    df.dropna(subset = ['outcome'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: clean_data\n",
    "\n",
    "Purpose: Transform yes and no into 1 and 0, remove some useless variables, drop variables \n",
    "less that 220 entries, only keep numerical variables\n",
    "\n",
    "Input: Dataframe with the data\n",
    "\n",
    "Output: Clean dataframe\n",
    "\n",
    "'''\n",
    "\n",
    "def clean_data(df):\n",
    "    \n",
    "    # Replace yes or no answers for 1 and 0\n",
    "    df = df.replace(\"sim\", int(1))\n",
    "    df = df.replace(\"Sim\", int(1))\n",
    "    df = df.replace(\"SIM\", int(1))\n",
    "    df = df.replace(\"s\", int(1))\n",
    "    df = df.replace(\"S\", int(1))\n",
    "    df = df.replace(\"yes\", int(1))\n",
    "    df = df.replace(\"Yes\", int(1))\n",
    "    df = df.replace(\"YES\", int(1))\n",
    "    df = df.replace(\"y\", int(1))\n",
    "    df = df.replace(\"Y\", int(1))\n",
    "    df = df.replace(\"nao\", int(0))\n",
    "    df = df.replace(\"Nao\", int(0))\n",
    "    df = df.replace(\"NAO\", int(0))\n",
    "    df = df.replace(\"não\", int(0))\n",
    "    df = df.replace(\"Não\", int(0))\n",
    "    df = df.replace(\"NÃO\", int(0))\n",
    "    df = df.replace(\"n\", int(0))\n",
    "    df = df.replace(\"N\", int(0))\n",
    "    df = df.replace(\"No\", int(0))\n",
    "    df = df.replace(\"no\", int(0))\n",
    "    df = df.replace(\"NO\", int(0))\n",
    "    \n",
    "    r = re.compile(\".*bks*\")\n",
    "    id_columns = list(filter(r.match, df.columns)) # Read Note\n",
    "    df.drop(columns=id_columns, inplace=True)\n",
    "    \n",
    "    r = re.compile(\"ID_.*\")\n",
    "    id_columns = list(filter(r.match, df.columns)) # Read Note\n",
    "    df.drop(columns=id_columns, inplace=True)\n",
    "    \n",
    "    r = re.compile(\"TMRAW.*\")\n",
    "    id_columns = list(filter(r.match, df.columns)) # Read Note\n",
    "    df.drop(columns=id_columns, inplace=True)\n",
    "    \n",
    "    r = re.compile(\"RESP.*\")\n",
    "    id_columns = list(filter(r.match, df.columns)) # Read Note\n",
    "    df.drop(columns=id_columns, inplace=True)\n",
    "    \n",
    "    r = re.compile(\"G05\")\n",
    "    id_columns = list(filter(r.match, df.columns)) # Read Note\n",
    "    df.drop(columns=id_columns, inplace=True)\n",
    "    \n",
    "    \n",
    "    df = df.apply(pd.to_numeric, errors='ignore')\n",
    "    df = df.select_dtypes(include=np.number)\n",
    "    \n",
    "    \n",
    "    \n",
    "    df.dropna(thresh=220, axis=1, inplace=True)\n",
    "    \n",
    "    for var in df.columns:\n",
    "        if df[var].nunique() == 1:\n",
    "            df.drop(columns=[var], inplace=True)\n",
    "    \n",
    "    print(\"\\nThe clean dataset has\", df.shape[0], \"rows and\", df.shape[1], \"columns\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: feature_variance_finder\n",
    "\n",
    "Purpose: Removing variables below a variance threshold\n",
    "\n",
    "Input: Dataframe with the data and desired variance threshold\n",
    "\n",
    "Output: Dataframe without the low variance features\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def feature_variance_finder(data, thresh):\n",
    "   \n",
    "    normalized = normalize(data)\n",
    "    data_scaled = pd.DataFrame(normalized)\n",
    "    \n",
    "    data_scaled.var()\n",
    "    \n",
    "    #storing the variance and name of variables\n",
    "    variance = data_scaled.var()\n",
    "    columns = data.columns\n",
    "    \n",
    "    #saving the names of variables having variance more than a threshold value\n",
    "\n",
    "    variable = [ ]\n",
    "\n",
    "    for i in range(0,len(variance)):\n",
    "        if variance[i]>=thresh: #setting the threshold as 1%\n",
    "            variable.append(columns[i])\n",
    "            \n",
    "    new_data = data[variable]\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open the excel file\n",
    "name= \"../data/Data_filtered_PROPER.csv\"\n",
    "df = pd.read_csv(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Split outcome 70/30 and drop any possible missing data\n",
    "df_70_30 = outcome_split(df, \"CSSA5.TOTAL\", 30, 70)\n",
    "\n",
    "df_70_30.dropna(inplace=True, axis=1)\n",
    "\n",
    "df_70_30.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 277)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check shape of resulting dataframe\n",
    "df_70_30.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop uninformative columns and columns that contain direct information about outcome\n",
    "df_70_30.drop(columns=[\"Unnamed: 0\", \"ID_continua.int\", \"CSSAimprove\", \"CSSA5.TOTAL\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The clean dataset has 240 rows and 261 columns\n"
     ]
    }
   ],
   "source": [
    "## Clean data converting all remaining variables into numerical values and dropping any that cannot be converted\n",
    "df_70_30 = clean_data(df_70_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove features with low variance\n",
    "df_70_30 = feature_variance_finder(df_70_30, 0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The clean dataset has 240 rows and 228 columns\n"
     ]
    }
   ],
   "source": [
    "## Clean data converting all remaining variables into numerical values and dropping any that cannot be converted\n",
    "df_70_30 = clean_data(df_70_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split train and test dataset 80/20 split\n",
    "train_70_30, test_70_30 = train_test_split(df_70_30, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 228)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check shape of train dataset\n",
    "train_70_30.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 228)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check shape of test dataset\n",
    "test_70_30.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save test and train dataset\n",
    "train_70_30.to_csv(\"../data/train_70_30.csv\", index=False)\n",
    "test_70_30.to_csv(\"../data/test_70_30.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
