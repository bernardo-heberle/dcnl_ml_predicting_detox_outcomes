{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c758cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "## Display all rows of pandas dataframes\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c049d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: outcome_split\n",
    "\n",
    "Purpose: Make the outcome variable binary, splitting it  within the percentiles\n",
    "of previously inputed low and high tails. \n",
    "\n",
    "Input: Dataframe, outcome variable name, low tail value, high tail value\n",
    "\n",
    "Output: Dataframe with \"outcome\" variable split in 1s and 0s as asked by the user\n",
    "\n",
    "'''\n",
    "\n",
    "def outcome_split(df, outcome, divider):\n",
    "    \n",
    "    ## Divide by divider given\n",
    "    constraint = df[outcome] <=divider\n",
    "    constraint2 = df[outcome] > divider\n",
    "    constraint3 = df[outcome].isna()\n",
    "\n",
    "\n",
    "    df.loc[constraint, 'outcome'] = int(0)\n",
    "    df.loc[constraint2, 'outcome'] = int(1)\n",
    "    df.loc[constraint3,'outcome'] = int(2)\n",
    "    \n",
    "    df.dropna(subset = ['outcome'], inplace=True)\n",
    "    \n",
    "    df.drop(columns=outcome, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7649af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: clean_data\n",
    "\n",
    "Purpose: Transform yes and no into 1 and 0, remove some useless variables, drop variables \n",
    "less that 220 entries, only keep numerical variables\n",
    "\n",
    "Input: Dataframe with the data\n",
    "\n",
    "Output: Clean dataframe\n",
    "\n",
    "'''\n",
    "\n",
    "def clean_data(df):\n",
    "    \n",
    "    # Replace yes or no answers for 1 and 0\n",
    "    df = df.replace(\"sim\", int(1))\n",
    "    df = df.replace(\"Sim\", int(1))\n",
    "    df = df.replace(\"SIM\", int(1))\n",
    "    df = df.replace(\"s\", int(1))\n",
    "    df = df.replace(\"S\", int(1))\n",
    "    df = df.replace(\"yes\", int(1))\n",
    "    df = df.replace(\"Yes\", int(1))\n",
    "    df = df.replace(\"YES\", int(1))\n",
    "    df = df.replace(\"y\", int(1))\n",
    "    df = df.replace(\"Y\", int(1))\n",
    "    df = df.replace(\"nao\", int(0))\n",
    "    df = df.replace(\"Nao\", int(0))\n",
    "    df = df.replace(\"NAO\", int(0))\n",
    "    df = df.replace(\"não\", int(0))\n",
    "    df = df.replace(\"Não\", int(0))\n",
    "    df = df.replace(\"NÃO\", int(0))\n",
    "    df = df.replace(\"n\", int(0))\n",
    "    df = df.replace(\"N\", int(0))\n",
    "    df = df.replace(\"No\", int(0))\n",
    "    df = df.replace(\"no\", int(0))\n",
    "    df = df.replace(\"NO\", int(0))\n",
    "    \n",
    "    r = re.compile(\".*bks*\")\n",
    "    id_columns = list(filter(r.match, df.columns)) # Read Note\n",
    "    df.drop(columns=id_columns, inplace=True)\n",
    "    \n",
    "    r = re.compile(\".*hrd*\")\n",
    "    id_columns = list(filter(r.match, df.columns)) # Read Note\n",
    "    df.drop(columns=id_columns, inplace=True)\n",
    "    \n",
    "    r = re.compile(\"ID_.*\")\n",
    "    id_columns = list(filter(r.match, df.columns)) # Read Note\n",
    "    df.drop(columns=id_columns, inplace=True)\n",
    "    \n",
    "    r = re.compile(\"TMRAW.*\")\n",
    "    id_columns = list(filter(r.match, df.columns)) # Read Note\n",
    "    df.drop(columns=id_columns, inplace=True)\n",
    "    \n",
    "    r = re.compile(\"RESP.*\")\n",
    "    id_columns = list(filter(r.match, df.columns)) # Read Note\n",
    "    df.drop(columns=id_columns, inplace=True)\n",
    "    \n",
    "    r = re.compile(\"G05\")\n",
    "    id_columns = list(filter(r.match, df.columns)) # Read Note\n",
    "    df.drop(columns=id_columns, inplace=True)\n",
    "    \n",
    "    \n",
    "    df = df.apply(pd.to_numeric, errors='ignore')\n",
    "    df = df.select_dtypes(include=np.number)\n",
    "    \n",
    "    \n",
    "    \n",
    "    df.dropna(thresh=220, axis=1, inplace=True)\n",
    "    \n",
    "    for var in df.columns:\n",
    "        if df[var].nunique() == 1:\n",
    "            df.drop(columns=[var], inplace=True)\n",
    "    \n",
    "    print(\"\\nThe clean dataset has\", df.shape[0], \"rows and\", df.shape[1], \"columns\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fbe77c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: feature_variance_finder\n",
    "\n",
    "Purpose: Removing variables below a variance threshold\n",
    "\n",
    "Input: Dataframe with the data and desired variance threshold\n",
    "\n",
    "Output: Dataframe without the low variance features\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def feature_variance_finder(data, thresh):\n",
    "   \n",
    "    normalized = normalize(data)\n",
    "    data_scaled = pd.DataFrame(normalized)\n",
    "    \n",
    "    data_scaled.var()\n",
    "    \n",
    "    #storing the variance and name of variables\n",
    "    variance = data_scaled.var()\n",
    "    columns = data.columns\n",
    "    \n",
    "    #saving the names of variables having variance more than a threshold value\n",
    "\n",
    "    variable = [ ]\n",
    "\n",
    "    for i in range(0,len(variance)):\n",
    "        if variance[i]>=thresh: #setting the threshold as 1%\n",
    "            variable.append(columns[i])\n",
    "            \n",
    "    new_data = data[variable]\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fd4d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open data, only keep women and remove problematic sleep problem variable and ID_continua.int\n",
    "df = pd.read_csv(\"../data/Data_240822_adj.csv\")\n",
    "df = df.loc[df[\"Gender\"] == \"woman\"]\n",
    "df.drop(columns=[\"P_SleepProblems_Life\", \"P_SleepProblemsLastTime\", \"ID_continua.int\", \"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b3de424",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dataset for binary classification analysis\n",
    "df_original = df.dropna(subset=['T_CTQ', 'ASI_Drugs', 'CSSA5.TOTAL', 'CSSA1.TOTAL']).copy()\n",
    "\n",
    "## Remove any missing data left\n",
    "df_original.dropna(inplace=True, axis=1)\n",
    "df_original.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bd48a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform outcome split using CSSA > 21 as high withdrawal outcome\n",
    "## Outcome 0: CSSA5.TOTAL <= 21\n",
    "## Outcome 1: CSSA5.TOTAL > 21\n",
    "df_original = outcome_split(df_original, \"CSSA5.TOTAL\", 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "519782e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dataset including outcome for people who left treatment\n",
    "df_dropouts = df.dropna(subset=['T_CTQ', 'ASI_Drugs', 'CSSA1.TOTAL']).copy()\n",
    "\n",
    "## Perform outcome split using CSSA > 21 as high withdrawal outcome\n",
    "## Outcome 0: CSSA5.TOTAL <= 21\n",
    "## Outcome 1: CSSA5.TOTAL > 21\n",
    "## Outcome 2: CSSA5.TOTAL is missing (patient did not complete detox treatment)\n",
    "df_dropouts = outcome_split(df_dropouts, \"CSSA5.TOTAL\", 21)\n",
    "\n",
    "## Remove any missing data left\n",
    "df_dropouts.dropna(inplace=True, axis=1)\n",
    "df_dropouts.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65464a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The clean dataset has 401 rows and 268 columns\n",
      "\n",
      "The clean dataset has 525 rows and 262 columns\n"
     ]
    }
   ],
   "source": [
    "## Convert variables to numeric, eliminate non-numerical variables and variables with no variation\n",
    "df_original = clean_data(df_original)\n",
    "\n",
    "df_dropouts = clean_data(df_dropouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42a74588",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove variables with very low variability\n",
    "df_original= feature_variance_finder(df_original, 0.0000001)\n",
    "\n",
    "df_dropouts = feature_variance_finder(df_dropouts, 0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2a327a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save files\n",
    "df_original.to_csv(\"../data/two_outcomes/whole_dataset.csv\", index=False)\n",
    "\n",
    "\n",
    "df_dropouts.to_csv(\"../data/three_outcomes/whole_dataset_dropouts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c47d6231",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create undersampled dataset\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority', random_state=27)\n",
    "\n",
    "X = df_original.drop(['outcome'], axis=1, inplace=False)\n",
    "y = df_original[\"outcome\"]\n",
    "\n",
    "X_under, y_under = undersample.fit_resample(X, y)\n",
    "\n",
    "df_original_under = X_under.copy()\n",
    "\n",
    "df_original_under[\"outcome\"] = y_under\n",
    "\n",
    "df_original_under.to_csv(\"../data/two_outcomes/undersampled_dataset.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
